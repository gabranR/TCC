{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a debugging file for the scraping actions\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second file: Scraping the Reviews\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/atrat_links.csv')\n",
    "links = data['Links']\n",
    "nomes_atr = data['Atrativos']\n",
    "#just saving this here for better readability\n",
    "nxt_css_select = '.cCnaz > div:nth-child(1) > a:nth-child(1) > svg:nth-child(1)'\n",
    "\n",
    "## Part 01: Getting the links and selecting the attractions ------------\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ToDo:\n",
    "GET:\n",
    "\n",
    "- a) the link to each profile for the ids\n",
    "- b) número de contribuições\n",
    "- c) nota (title da tag svg)\n",
    "- d) data da avaliação\n",
    "- e) data da viagem\n",
    "- f) local de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Successifuri\n",
      "2\n",
      "Completed Successifuri\n",
      "2\n",
      "Completed Successifuri\n",
      "2\n",
      "Completed Successifuri\n",
      "2\n",
      "Completed Successifuri\n",
      "2\n",
      "Completed Successifuri\n",
      "2\n",
      "Completed Successifuri\n",
      "We've reached the last page\n"
     ]
    }
   ],
   "source": [
    "option = Options()\n",
    "option.headless = True\n",
    "with  webdriver.Firefox(options=option) as driver:\n",
    "\n",
    "\n",
    "    ## Gettiing da cookie... the smart way --- \n",
    "    driver.get(links[57])\n",
    "    time.sleep(3)\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.find_element_by_xpath('//*[@id=\"onetrust-pc-btn-handler\"]').click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath('/html/body/div[2]/div[4]/div[3]/div[1]/button[1]').click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # click in 'all languagesss'\n",
    "\n",
    "    driver.find_element_by_xpath('/html/body/div[1]/main/div[1]/div[3]/div[2]/div/div/span/section[8]/div/div/span/section/section/div[1]/div/div[1]/span/div/div[2]/div/div/span[2]/span/div/div/button').click()\n",
    "    time.sleep(2)                \n",
    "    driver.find_element_by_xpath('//*[@id=\"menu-item-all\"]').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    ##uhhhh so fancy using classes... OOP uhhh =======\n",
    "    # The ideia here is to create a commom set of attributes that can be used by the methods, and then I can create objects requesting specific tags\n",
    "\n",
    "    class Scraper:\n",
    "        def __init__(self, dict_):\n",
    "            self.dict = dict_\n",
    "        # Just a function to return false when there is no 'next' button anymore\n",
    "        def stop_pag(self):\n",
    "            temp = bool(driver.find_elements_by_css_selector(nxt_css_select))\n",
    "            return temp\n",
    "        \n",
    "    \n",
    "        ## Função para encontrar *os elementos*, salvar o html, parsear o html e retornar uma lista com o texto do elemento\n",
    "        # no momento suporta texto e href, mas caso precise de outra informação é só incluir mais um elif\n",
    "\n",
    "        # for every element in elements I should locate, parse and copy ALL the info i'll need\n",
    "        def find_them(self):\n",
    "            elements = driver.find_elements_by_css_selector('div.ffbzW')\n",
    "            a = []\n",
    "            b = []\n",
    "            #df = pd.DataFrame({'User_ID': [], 'Data': []})\n",
    "            for element in elements:\n",
    "                html = element.get_attribute('innerHTML')\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "                a.append(soup.find(self.dict['user_id']['tag'], self.dict['user_id']['class'])['href'])\n",
    "                temp = soup.find(self.dict['date']['tag'], self.dict['date']['class'])\n",
    "                b.append(temp.text if temp else 'NA')\n",
    " \n",
    "            #df = pd.concat( objs = [df, pd.DataFrame([a, b])])\n",
    "            #print(len(a))\n",
    "            print('Completed Successifuri')\n",
    "            return [a, b]\n",
    "\n",
    "\n",
    "        ## A METHOD TO RULE THEM ALL =====\n",
    "        # This method creates the iteration of the action of clicking on the next button, and also extends the list - consolidates - all info\n",
    "        def give_me_info(self):\n",
    "            temp = [[], []]\n",
    "            while self.stop_pag():\n",
    "                temp2 = self.find_them()\n",
    "                print(len(temp2))\n",
    "                temp[0].extend(temp2[0])\n",
    "                temp[1].extend(temp2[1])\n",
    "                #print(len(temp))\n",
    "                driver.find_element_by_css_selector(nxt_css_select).click()\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                temp3 = self.find_them()\n",
    "                temp[0].extend(temp3[0])\n",
    "                temp[1].extend(temp3[1])\n",
    "            \n",
    "            df = pd.DataFrame(data = {'User_ID': temp[0], 'Data': temp[1]})\n",
    "            df.to_csv('data/info_test.csv', mode = 'a', index = False)\n",
    "            print('We\\'ve reached the last page')\n",
    "\n",
    "    \n",
    "    dict_1 = {\n",
    "        'user_id': {\n",
    "            'tag': 'a',\n",
    "            'class': 'iPqaD _F G- ddFHE eKwUx btBEK fUpii',\n",
    "            'attr': 'href'\n",
    "        },\n",
    "        \n",
    "        'date': {\n",
    "            'tag': 'div',\n",
    "            'class': 'eRduX',\n",
    "            'attr': 'text'\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    scr_1 = Scraper(dict_1)\n",
    "\n",
    "    aaaaa = scr_1.give_me_info()\n",
    "\n",
    "\n",
    "###### AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ######\n",
    "## The code works! But it's ugly as hell - also it is taking about a minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.tripadvisor.com.br/Attraction_Review-g303536-d7011428-Reviews-Museu_Naval-Gramado_State_of_Rio_Grande_do_Sul.html'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "links[57]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3cef28238c25641b9601de891cf85fa5f544bfe0130becece889e10fbca7396"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
